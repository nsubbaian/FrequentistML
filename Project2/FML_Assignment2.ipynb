{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FML_Assignment2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMY8BGy4tKP1SdehCvEWvS0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nsubbaian/FrequentistML/blob/master/Project2/FML_Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JWTxB9UmnVH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Assignment 2:  Stochastic Gradient Descent\n",
        "Read sections: 4.1, 4.3 (not 4.3.1 and 4.3.2), 4.4-4.4.2  and the following paper:\n",
        "https://leon.bottou.org/publications/pdf/compstat-2010.pdf\n",
        "Grab a binary classification dataset from UCI or other repository. Divide your data into roughly 80% train, 10% validation, 10% test. Implement logistic regression with stochastic gradient descent as the optimization algorithm.\n",
        "Implement SGD without regularization and report your % correct on the test dataset.\n",
        "Implement SGD with regularization, select the best lambda parameter using the validation dataset, and report your % correct on the test dataset.\n",
        "Plot the likelihood function with respect to iterations for unregularized and regularized on one set of axes. Which one converges to a higher likelihood, and why?\n",
        "Optional, advanced things to try:\n",
        "- Implement SGD with the L-1 penalty and use it for feature selection (it is not that hard actually)\n",
        "- Compare SGD to Newton-Raphson by plotting the likelihood of both on one set of axes and explain why they are different."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}